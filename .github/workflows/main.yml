name: Update Parquet File in Google Cloud Storage

on:
  workflow_dispatch:  # Allows manual triggering
  schedule:
    - cron: '0 0 * * *'  # Runs daily at midnight

jobs:
  update_parquet_file:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'  # Use the desired Python version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install google-cloud-storage pandas pyarrow  # Required libraries

      - name: Configure Google Cloud credentials
        env:
          GOOGLE_APPLICATION_CREDENTIALS: "${{ runner.temp }}/gcp-key.json"
        run: |
          echo "${{ secrets.GCP_CREDENTIALS }}" > $GOOGLE_APPLICATION_CREDENTIALS

      - name: Run Python script to update Parquet file in GCS
        env:
          GOOGLE_APPLICATION_CREDENTIALS: "${{ runner.temp }}/gcp-key.json"  # Set the environment variable for Google Auth
          BUCKET_NAME: ${{ secrets.GCP_BUCKET_NAME }}
          OBJECT_KEY: ${{ secrets.GCP_OBJECT_KEY }}
          PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        run: |
          python - <<EOF
          import os
          from google.cloud import storage
          import pandas as pd
          import pyarrow as pa
          import pyarrow.parquet as pq
          from io import BytesIO

          # Google Cloud Storage configuration from environment variables
          bucket_name = os.getenv("BUCKET_NAME")
          object_key = os.getenv("OBJECT_KEY")
          project_id = os.getenv("PROJECT_ID")

          # Initialize Google Cloud Storage client
          client = storage.Client(project=project_id)
          bucket = client.bucket(bucket_name)
          blob = bucket.blob(object_key)

          # Download the Parquet file
          parquet_data = blob.download_as_bytes()
          df = pd.read_parquet(BytesIO(parquet_data))

          # Perform any updates on the DataFrame (example)
          # df['new_column'] = 'example_value'

          # Save updated DataFrame to Parquet and upload back to GCS
          output_buffer = BytesIO()
          df.to_parquet(output_buffer, index=False)
          output_buffer.seek(0)
          blob.upload_from_file(output_buffer, content_type='application/octet-stream')
          print(f"Parquet file updated and uploaded to {object_key} in {bucket_name} bucket.")
          EOF
